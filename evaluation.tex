

\section{Evaluations} \label{Evaluation}

\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figs/architecture/Topology.jpg}
	\caption{\small The network consists of 8 PoPs with programmable switches highlighted in color, and 8 PoPs without programmable switches marked in white, which are connected by a backbone network of 40 links.}
	\label{fig:topo}
\end{figure}

We evaluate the result through the topology, as shown in Figure \ref{fig:topo}, includes 860 {\egresses}, among which there are 32 Transit {\egresses}, comprising 16 PoPs in total. 8 of these PoPs have programmable devices capable of detecting flows, while the other 8 PoPs act solely as transit PoPs, utilizing their backbone network links. The total number of backbone network links is 40. The amount of flows in the test input is approximately 2 million per time slot and the flows are all from real business traffic of 1 week (from May 22, 2023 to May 28, 2023). Performance-aware flows are marked according to the application service class, with the number being roughly 4\% of the total number of flows. The algorithm runs on a single server with 64GB of memory and 32 CPU cores.


% performance-aware flow identification and characteristics. 

There are two parameters to be determined. We filter non-performance-aware small flows based on a flow filter threshold $FT$. 
Considering the risk in practical deployments, we set the maximum percentage of {\egress} burst capacity as a hyperparameter burst threshold, denoted as $BT$, to prevent congestion at the {\egresses}.

% 其他论文的用词
% 冲顶 ！= burst
We compare the difference in scheduling results between {\sys} and the existing network scheduling algorithm in terms of cost optimization and performance optimization. Existing approaches select the optimial egress port based on the monitoring results.   This baseline method switch reroutes the entire traffic trunks associated with the specified ASes from one {\egress} to another. However, the routing granularity is coarse \todo{dest AS and thus the baseline does not optimize the final cost.} We will show that {\sys} employs application service classes and destination ISP prefixes to identify flows, thereby optimizing final cost and performance-aware routing to better align with the specific performance requirements of clients.

To verify the effectiveness of the proposed algorithms. We first conduct tests on our algorithm's performance in cost optimization and performance optimization under various hyperparameters. Then we evaluate the algorithm's runtime under different numbers of threads in large-scale tests to show that it has good scalability. Finally, we discuss problems that may be encountered in applying the algorithm.

% Nowadays, cloud providers consistently implement performance-aware routing at cloud's edge. The baseline approach categorizes flows directed towards specific predetermined ASes as performance-sensitive flows. This baseline method switch reroutes the entire traffic trunks associated with the specified ASes from one {\egress} to another. However, this routing granularity is relatively coarse.  Conversely, our proposed approach employs source application types and destination prefixes to identify flows, thereby optimizing performance-aware routing to better align with the specific performance requirements of clients.


% We select the baseline algorithm ... briefly explains the algorithm and reference \cite{iwqos, infocom}. % 给几个NSDI，SIGCOMM iwqos 
% We compare the scheduling results of {\sys} with the existing network controller’s traffic scheduling algorithm. 


\subsection{Cost Optimization} \label{Cost Optimization}

We first conduct offline tests for 1 to 7 days with hyperparameters such as a burst threshold of 90\% and a flow filter threshold of 0.1Mb. The test results are shown in Table \ref{tab:7days}, indicating that {\sys} exhibits relatively stable cost optimization compared to the existing network controller's traffic scheduling scheme, with an optimization ratio of approximately 36\%. Furthermore, as the number of time slots increases, {\sys} achieves greater cost optimization, suggesting that our algorithm possesses a certain level of stability. Given that the effect of cost optimization is almost independent of the number of test slots, the number of test slots is set to the number of slots for 1 day, i.e. 288 slots, for all subsequent experimental tests.

\begin{table}[tbp]
%	\footnotesize
	\centering
  \resizebox{0.48\textwidth}{7mm}{
	\begin{tabular}{c|c|c|c|c|c|c|c}
		\hline
 Days & 1 & 2 & 3 & 4 & 5 & 6 & 7  \\ 
 \hline
 Baseline & 1080.09 &	1106.51 & 1137.02 & 1162.89 & 1173.52 & 1181.04 & 1187.20\\ 
 \hline
 {\sys}& 712.47 & 719.21 & 730.95 & 737.69 & 744.79 & 750.27 & 754.18  \\ 
 \hline
 Optimization & 34.04\% & 35.00\% & 35.71\% & 36.56\% &36.53\% & 36.47\% & 36.47\% \\ 
 \hline
	\end{tabular}
 }
        \vspace{-0.1in}
	\caption{\small Cost optimization for 1 to 7 days of offline testing under a 90\% burst threshold and a 0.1Mb flow filter threshold}
	\label{tab:7days}
\end{table}

We compared the cost optimization effects of {\sys} under different hyperparameter settings to the existing network scheduling algorithm while keeping other experimental settings constant. Figure \ref{fig:costTestpara1} illustrates the relationship between the flow filter threshold and the percentage of cost savings under the same burst threshold. It shows that with a fixed burst threshold, the percentage of cost savings increases as the flow filter threshold decreases. However, when the flow filter threshold becomes sufficiently small (e.g., 0.1Mb), further decreasing the flow filter threshold does not significantly increase the final cost. It indicates that higher burst thresholds often result in a lower final cost. This is because the total bandwidth of the remaining flows does not decrease significantly as FT continues to decrease, thereby not yielding a substantial enhancement in cost optimization.

\begin{figure}
	\centering
	\includegraphics[width = 7cm]{figs/evaluation/costTestpara1.pdf}
	\caption{\small Impact of flow filter threshold on cost optimization when choosing different burst thresholds}
	\label{fig:costTestpara1}
\end{figure}


% \begin{figure}
% 	\centering
% 	\includegraphics[width=8cm]{figs/evaluation/plot2.pdf}
% 	\caption{\small test SVG figure}
% 	\label{fig:test}
% \end{figure}


\subsection{Performance Optimization}
Then, we test the performance optimization effect of {\sys} on performance-aware flows. We choose the delay of the flow to the {\egress} as the performance metric, which contains the {\egress} delay, and the backbone link delay. We compare the differences in latency performance optimization between the {\sys} scheduling algorithm and the existing network controller's scheduling algorithm for performance-aware flows. Our testbed settings are the same as mentioned earlier. We test for 288 time slots, with a burst threshold of 90\% and a flow filter threshold of 0.1Mb. Figure \ref{fig:performanceTestAbs} illustrates the variation of average latency for performance-aware flows over time slots under the two traffic scheduling schemes. {\sys}, compared to the existing network's traffic scheduling algorithm, reduces the average latency for performance-aware flows from 44.2ms to 41.7ms, achieving an absolute average decrease of about 2.5ms. We test the percentage of performance-aware flows optimized by {\sys} to the total performance-aware flows under each time slot. As shown in Figure \ref{fig:performanceTestPrecent}, in comparison to the existing network controller's scheduling algorithm, {\sys} can reduce the latency of approximately 67\% performance-aware flows.
We achieve the similar relusts by using different parameters $FT=\{0.01, 0.1,0.5,1,5\}$Mbps and $BT=\{100\%, 90\%, 80\%\}$. This is due to three main reasons: Firstly, {\sys} does not filter the performance-aware flows, which means the flow filter threshold is unaffected by the performance. Secondly, our algorithm prioritizes optimizing the performance for performance-aware flows by a heuristic strategy; Thirdly, the number of performance-aware flows is relatively small and constitutes a small portion of the total flow size. So almost every performance-aware flow can be placed to the optimal {\egress}.

\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figs/evaluation/performanceTestAbs.pdf}
	\caption{\small Comparison of average delay of performance-aware flows at each time slot under {\sys} and existing network scheduling algorithm}
	\label{fig:performanceTestAbs}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figs/evaluation/performanceTestPrecent.pdf}
	\caption{\small Percentage of optimized performance-aware flows to total performance-aware flows at each time slot with {\sys} compare to existing network scheduling algorithm}
	\label{fig:performanceTestPrecent}
\end{figure}





\iffalse
\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figs/evaluation/performanceTestHyperparameters.png}
	\caption{\small Impact of choosing different hyperparameters on performance optimization}
	\label{fig:performanceTestHyperparameters}
\end{figure}


\begin{table}[tbp]
	\footnotesize
	\centering
{
	\begin{tabular}{c|c|c|c}
		\hline
 \diagbox{FT}{BT} & 100\% & 90\% & 80\%  \\ 
 \hline \hline 
 0.01 Mb & 66.89\% & 66.89\% & 66.89\%  \\ 
 \hline
 0.1 Mb  & 66.89\% & 66.89\% & 66.89\%  \\ 
 \hline
  0.5 Mb & 66.88\% & 66.88\% & 66.88\%  \\ 
   \hline
  1 Mb   & 66.90\% & 66.88\% & 66.90\%  \\ 
   \hline
  5 Mb   & 66.89\% & 66.89\% & 66.89\%  \\ 
 \hline
 \end{tabular}
 }
     \vspace{-0.1in}
	\caption{\small The ratio of performance-aware flows optimized for different hyperparameter combinations to the total number of performance-aware flows}
	\label{fig:performanceTestHyperparameters}
\end{table}

The results are shown in Table \ref{fig:performanceTestHyperparameters}. It shows that different hyperparameters do not significantly affect the performance optimization for performance-aware flows. This is due to three main reasons: Firstly, {\sys} does not filter the performance-aware flows, which means the flow filter threshold is unaffected by the performance. Secondly, our algorithm prioritizes optimizing the performance for performance-aware flows by a heuristic strategy; Thirdly, the number of performance-aware flows is relatively small and constitutes a small portion of the total flow size. So almost every performance-aware flow can be placed to the optimal {\egress}.

\fi

% 我们的算法在更大规模下是不是还适用
\subsection{Scalability}
Since {\sys}'s objective is to perform large-scale traffic scheduling, computational efficiency during online scheduling is of paramount importance. Following the same testbed setup as described earlier, we test the average runtime of the algorithm per time slot under different hyperparameters for the serial version. Table \ref{fig:runtimeHyper} shows that the burst threshold has almost no impact on the average runtime, as the average runtime is primarily determined by the large number of flows awaiting scheduling. On the other hand, changes in the flow filter threshold significantly impact the average runtime of the algorithm. As the flow filter threshold increases, fewer flows participate in the optimization schedule, resulting in shorter algorithm runtime.

\begin{table}[tbp]
%	\footnotesize
	\centering
  \resizebox{0.48\textwidth}{13mm}{
	\begin{tabular}{c|c|c|c|c|c}
		\hline
 FT & BW PCT. & Number PCT. & BT=100\% & BT=90\% & BT=80\% \\ 
 \hline \hline 
 0 Mb & 100\% & 100\% & 30073.82 & 31720.58 & 29794.35 \\ 
 \hline
 0.01 Mb & 99.63\% & 13.33\% & 4734.45 & 4426.03 & 4479.67 \\ 
 \hline
 0.1 Mb & 99.22\% & 3.67\% & 1910.24 & 2074.08 & 1927.25 \\ 
 \hline
  0.5 Mb & 96.44\% & 1.42\% & 1794.02 & 1889.40 & 1764.48 \\ 
   \hline
  1 Mb & 89.10\% & 0.92\% & 1775.68 & 1767.36 & 1684.17 \\ 
   \hline
  5 Mb & 86.49\% & 0.35\% & 1652.33 & 1548.39 & 1586.25 \\ 
 \hline
 \end{tabular}
 }    
    \vspace{-0.1in}
	\caption{\small The impact of varying hyperparameters on the algorithm's runtime under a single thread (in milliseconds), with "BW PCT." representing the percentage of remaining bandwidth after filtering and "Number PCT." denoting the percentage of the number of flows remaining after filtering.}
	\label{fig:runtimeHyper}
\end{table}


Next, we scale up the number of flows by triplicating the flows, bringing the flow number to around 5 million. The bandwidths of these flows are reduced to one-third of the original to keep the total bandwidth the same. Besides, no flows are filtered, maintaining a flow count of around 5 million for scheduling, while other experimental settings remain consistent. We compare the average runtime of the algorithm per time slot under different numbers of threads. As shown in Figure \ref{fig:runtimeComp}, under the parallel 32-threaded mode, the algorithm's average runtime is approximately 80\% shorter than the serial mode. This indicates that if filtering is applied to flows with a flow filter threshold of 0.1Mb, our algorithm can support the scheduling of around 500 million flows within approximately 10 seconds with 32 threads.

% \begin{table}[tbp]
% %	\footnotesize
% 	\centering
%   \resizebox{0.48\textwidth}{7mm}{
% 	\begin{tabular}{c|c|c|c|c|c|c}
% 		\hline
%  Mode & Burst & Delay-Sensetive Flow & Inter-PoP & Intra-PoP & Remain Flows & Total \\ 
%  \hline \hline 
%  32 Parallel threads & 90.40 & 57.62 & 528.42 & 3356.14 & 6004.58 & 10037.16 \\ 
%  \hline
%  2 Parallel threads  & 90.28 & 56.55 & 1353.64 & 46104.40 & 15756.87 & 63361.74 \\ 
%  \hline
%  Serial & 96.82 & 41.88 & 44011.70 & 44011.70 & 11618.94 & 55769.34 \\ 
%  \hline
%  \end{tabular}
%  }
%         \vspace{-0.1in}
% 	\caption{\small Running time of individual components in different run modes(ms)}
% 	\label{fig:runtimeComp}
% \end{table}
\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figs/evaluation/runtimeComp.pdf}
	\caption{\small Running time per time slot of different number of threads under about 5 million flows(s)}
	\label{fig:runtimeComp}
\end{figure}


\subsection{Routing Table Compression}
\begin{table}[tbp]
	%	\footnotesize
	\centering
	\resizebox{0.48\textwidth}{7mm}{
		\begin{tabular}{c|c|c}
			\hline
			Memory (MB) & Prefix Index & Routing Index  \\ 
			\hline
			Baseline (Google RadixTree\cite{googleradixtree}) & 204.37 &	556.48 \\ 
			\hline
			{\sys}& 11.55 & 95.31   \\ 
			\hline
			Optimization &  17.69$\times$ & 5.93$\times$ \\ 
			\hline
		\end{tabular}
	}
	\vspace{-0.1in}
	\caption{\small Memory optimization of \emph{ForestBitmap}}
	\label{tab:compression-mem}
\end{table}

% Google Radix Tree: https://code.google.com/archive/p/radixtree/

\begin{table}[tbp]
	%	\footnotesize
	\centering
	\resizebox{0.48\textwidth}{7mm}{
		\begin{tabular}{c|c|c|c|c|c|c|c}
			\hline
			Time (ms) & Index Build & Longest Match & Full Match & Subnet  Match & Delete & Origin\_as Filter & Update  \\ 
			\hline
			Baseline (Google RadixTree\cite{googleradixtree}) & 1534 &	4.463 & 8.251 & 758.3 & 122.6 & 75.44 & 1306 \\ 
			\hline
			{\sys}& 533.8 & 1.453 & 2.554 & 39.31 & 25.76 & 55.79 & 81.31 \\ 
			\hline
			Optimization & 2.87$\times$ & 3.07$\times$ & 3.23$\times$ & 19.29$\times$ & 4.75$\times$ & 1.35$\times$ & 16.06$\times$ \\ 
			\hline
		\end{tabular}
	}
	\vspace{-0.1in}
	\caption{\small Routing computation optimization of \emph{ForestBitmap}}
	\label{tab:compression-time}
\end{table}

The results obtained from the live network data set demonstrate that our \emph{ForestBitmap} achieves a compression ratio of 17.69 times when compared to the baseline for pure prefix index, as presented in Table \ref{tab:compression-mem}. For the complete routing table index, \emph{ForestBitmap} achieves a compression ratio of 5.83 times in Table \ref{tab:compression-mem} and a computing efficiency improvement from 1.35 to 19.29 times in Table \ref{tab:compression-time}.







\subsection{Discussion}
\para{How to set the hyperparameters?} Based on the results of the tests mentioned above, we analyzed the impact of the two hyperparameters, the burst threshold $BT$ and the flow filter threshold $FT$, on the final optimization results. Cost optimization is influenced by both hyperparameters, while performance optimization is unaffected by them, and the algorithm's average runtime is influenced solely by the flow filter threshold. To maintain favorable cost optimization results and mitigate the risks associated with the upper bound of {\egress} capacity, we ultimately set the burst threshold to 90\%. Figure \ref{fig:hyperChoose} shows the percentage of cost savings and the algorithm's average runtime under different flow filter thresholds. It can be said that as the flow filter threshold decreases from 5Mb to 0.1Mb, the final cost significantly reduces, while the algorithm's average runtime remains relatively stable. However, when the flow filter threshold is further reduced to 0.01Mb, the percentage of cost savings doesn't exhibit significant growth, while the algorithm's average runtime increases notably. Consequently, we settled on a flow filter threshold of 0.1Mb.

\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figs/evaluation/hyperChoose.pdf}
	\caption{\small Percentage of cost savings and average running time of a single time slot under a single thread for different flow filter thresholds at 90\% of the burst threshold}
	\label{fig:hyperChoose}
\end{figure}

\para{What if the number of performance-aware flows is large?} When performance-aware flows dominate the bandwidth of the majority of flows in the network, {\sys} can still integrate cost and performance objectives for traffic scheduling. The reason is that performance-aware flows often tend to be scheduled within their original PoPs for better performance. Scheduling across PoPs may impair performance metrics such as delay and packet loss. However, the core of cost optimization lies in making full use of the suggested bandwidth of the virtual link capacity between PoPs given by the solver. Therefore, if non-performance-aware flows can fully utilize the capacity of these virtual links, the final cost can still be optimized by {\sys}. If the total available capacity of each intra-PoP {\egress} is greater than the total bandwidth of the performance-aware flows within the PoP, the performance of flows can still be optimized.

\para{What if the virtual link capacity is not utilized during heuristic scheduling?} We designed another simple heuristic scheduling algorithm to demonstrate the necessity of utilizing virtual link capacity. This algorithm schedules flows directly to a feasible {\egress} after arranging them in descending order of bandwidth. We evaluate the performance of this simple algorithm and {\sys} in terms of cost optimization. As shown in Figure \ref{fig:backboneEffect}, this heuristic algorithm can only optimize approximately 6\% of the cost, which is significantly lower than the optimization achieved by {\sys}. As analyzed in Section \ref{Solving Optimization Quickly}, the insufficient capacity of the backbone network results in the inability to route flows out of certain PoPs, causing rapid growth in the {\egress} $p_i$ of these PoPs.

\begin{figure}
	\centering
	\includegraphics[width = 8cm]{figs/evaluation/backboneEffect.pdf}
	\caption{\small Comparing cost optimization between {\sys} and the simple heuristic algorithms using the hyperparameter settings $FT=0.1Mb$ and $BT=90\%$.}
	\label{fig:backboneEffect}
\end{figure}

% \para{What if the predicted charged bandwidth per {\egress} given by the overall level optimization is not accurate?} Since the {\egress} charge bandwidth prediction $p_i$ given by the overall level optimization will act as a constraint when scheduling online, if the sum of the $p_i$ of all {\egresses} within a PoP is too low, 5\% of the burst time slots will be quickly exhausted. In this case, the actual final cost may be much higher than the overall level prediction.


% \subsection{Robustness Analyze}
% \subsection{Tolerance}  Error Resilience Analysis 或者 Error Tolerance Analysis  叫作误差容忍性分析
% 分析对于预测的敏感程度 和 预测偏差的容忍度
% 1. 如果总体层面不那么准
% 2. 如果预测的总带宽和PoP的带宽趋势不那么准（每个月的带宽增加是在这里，但如果预测的和这个月的带宽增加不一样的，那就要查看这个）
% 3. 如果时隙层面的流不那么准
% 4. 如果时隙层面的PoP带宽直接用上一个时隙的

% 如果有突发的大流，如何处理不会使得成本 prohibitively raise
% \subsection{Overall Optimization Effectiveness Analysis}
% 模块的有效性分析

